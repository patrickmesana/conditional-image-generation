C:\Anaconda2\envs\deeplearning\python.exe C:/Dev/conditional-image-generation/convolution.py
Using Theano backend.
Using cuDNN version 5110 on context None
Mapped name None to device cuda: GeForce GTX 1070 (0000:01:00.0)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 64, 64, 3)         0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 64, 64, 16)        448       
_________________________________________________________________
activation_1 (Activation)    (None, 64, 64, 16)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 64, 64, 16)        64        
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 64, 16)        2320      
_________________________________________________________________
activation_2 (Activation)    (None, 64, 64, 16)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 32, 32, 16)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 32, 32)        4640      
_________________________________________________________________
activation_3 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248      
_________________________________________________________________
activation_4 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
activation_5 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
activation_6 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 32, 32, 64)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 32, 32, 32)        18464     
_________________________________________________________________
activation_7 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      
_________________________________________________________________
activation_8 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 64, 64, 32)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 64, 64, 16)        4624      
_________________________________________________________________
activation_9 (Activation)    (None, 64, 64, 16)        0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 64, 64, 16)        64        
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 64, 64, 16)        2320      
_________________________________________________________________
activation_10 (Activation)   (None, 64, 64, 16)        0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 64, 64, 16)        64        
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 64, 64, 3)         435       
=================================================================
Total params: 107,875
Trainable params: 107,523
Non-trainable params: 352
_________________________________________________________________
Train on 82611 samples, validate on 40438 samples
Epoch 1/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5799 - acc: 0.0210 Epoch 00000: val_loss improved from inf to 0.66026, saving model to ./weights.hdf5
82611/82611 [==============================] - 3406s - loss: 0.5798 - acc: 0.0210 - val_loss: 0.6603 - val_acc: 0.0196
Epoch 2/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5601 - acc: 0.0215 Epoch 00001: val_loss did not improve
82611/82611 [==============================] - 3394s - loss: 0.5601 - acc: 0.0215 - val_loss: 0.7727 - val_acc: 0.0204
Epoch 3/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5573 - acc: 0.0215 Epoch 00002: val_loss did not improve
82611/82611 [==============================] - 3394s - loss: 0.5573 - acc: 0.0215 - val_loss: 0.7521 - val_acc: 0.0206
Epoch 4/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5558 - acc: 0.0216 Epoch 00003: val_loss did not improve
82611/82611 [==============================] - 3393s - loss: 0.5558 - acc: 0.0216 - val_loss: 0.6876 - val_acc: 0.0208
Epoch 5/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5548 - acc: 0.0216 Epoch 00004: val_loss improved from 0.66026 to 0.62344, saving model to ./weights.hdf5
82611/82611 [==============================] - 3392s - loss: 0.5548 - acc: 0.0216 - val_loss: 0.6234 - val_acc: 0.0210
Epoch 6/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5541 - acc: 0.0216 Epoch 00005: val_loss improved from 0.62344 to 0.59484, saving model to ./weights.hdf5
82611/82611 [==============================] - 3393s - loss: 0.5541 - acc: 0.0216 - val_loss: 0.5948 - val_acc: 0.0210
Epoch 7/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5534 - acc: 0.0216 Epoch 00006: val_loss did not improve
82611/82611 [==============================] - 3393s - loss: 0.5534 - acc: 0.0216 - val_loss: 0.5997 - val_acc: 0.0211
Epoch 8/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5529 - acc: 0.0216 Epoch 00007: val_loss improved from 0.59484 to 0.58130, saving model to ./weights.hdf5
82611/82611 [==============================] - 3393s - loss: 0.5529 - acc: 0.0216 - val_loss: 0.5813 - val_acc: 0.0211
Epoch 9/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5524 - acc: 0.0216 Epoch 00008: val_loss did not improve
82611/82611 [==============================] - 3394s - loss: 0.5524 - acc: 0.0216 - val_loss: 0.6327 - val_acc: 0.0210
Epoch 10/100
82400/82611 [============================>.] - ETA: 8s - loss: 0.5520 - acc: 0.0216 Epoch 00009: val_loss did not improve
82611/82611 [==============================] - 3399s - loss: 0.5520 - acc: 0.0216 - val_loss: 0.6013 - val_acc: 0.0210